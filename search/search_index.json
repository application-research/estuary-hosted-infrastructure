{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Filecoin Data Infrastructure","text":"<p>Welcome!</p>"},{"location":"#basics","title":"Basics","text":"<p>Basics covers some basics of operating and managing FDI, and common tasks and things you may want to do or monitor.</p> <p>It covers the following topics: Connecting to FDI (Netbird), Proxmox (Groundfloor), staff desktops, DNS, DHCP, machine booting, monitoring, backups, security &amp; patching.</p>"},{"location":"#databases","title":"Databases","text":"<p>Databases contains a list of PostgreSQL databases and where they exist in the infrastructure, for both VM-hosted and Kubernetes hosted (\"Postgres Operator\") databases.</p>"},{"location":"#kubernetes-and-cpi","title":"Kubernetes and CPI","text":"<p>The Kubernetes section covers: Logging into Kubernetes, getting Kubectl access, and managing Customer Provisioned Infrastructure (managing customers, creating new customers, editing existing customers)</p>"},{"location":"#services","title":"Services","text":"<p>Services covers the various services hosted in FDI or that allow one to host things using FDI.</p> <p>These include: Rancher, AWX, EstuaryV2 - Edge, Delta, Edge-URID, Carriers, Shuttle 12, API node</p>"},{"location":"#storage","title":"Storage","text":"<p>Storage covers the four main storage systems in use at FDI, and how to care and feed for them.</p> <p>These include: ZFS (ShuttleRescue), MooseFS, GarageHQ and Ceph.</p>"},{"location":"#rescue","title":"Rescue","text":"<p>Rescue covers accessing the infrastructure when All Hope Is Lost, when you need to access things with no other easy way in, or when a physical Groundfloor node is having issues. It covers accessing machines using IPMI (out of band access for when the OS is not responding) and dialing in using Wireguard (for when the Netbird Bastion hosts are unavailable)</p>"},{"location":"#tls","title":"TLS","text":"<p>TLS covers all things certificates, and troubleshooting steps for rotating certificates and the like.</p> <ul> <li>Managing TLS - Rotating TLS throughout the infrastructure with Wildcard-TLS-Playbook<ul> <li>Keeping certificates up to date</li> <li>Adding new hosts to distribute TLS certificates to</li> </ul> </li> <li>Managing TLS - Self-signed TLS for PostgreSQL clusters (etcd)</li> </ul>"},{"location":"#general-and-specialised-guides","title":"General and Specialised Guides","text":"<p>These are guides that don't (yet) fit in elsewhere in the documentation.</p> <ul> <li>Diagnosing Pacemaker clusters including Prod HAProxy</li> <li>Ceph: Rebalancing Ceph OSDs</li> <li>Kubernetes: Fixing stuck Rook.io volumes which are preventing containers from starting</li> </ul>"},{"location":"about/","title":"Intro","text":"<p>Filecoin Data Infrastructure is the physical and digital infrastructure that powers Filecoin Data Tools.</p> <p>It is a number of things:</p> <ul> <li>an open source project and blueprint for SPs and other parties that allows you to build your own private cloud (\"an FDI\")</li> <li>a hosting solution for all FDT customers and Estuary infrastructure</li> </ul>"},{"location":"about/#open-source","title":"Open source","text":"<p>Yes, we're open source. You can find the code in the following places, mainly:</p>"},{"location":"about/#core-repos","title":"Core repos","text":"<p>Here are the core repositories that make up FDI</p> <ul> <li>https://github.com/application-research - Main GitHub organisation</li> <li>https://github.com/application-research/estuary-hosted-infrastructure - Main \"EHI\" repo, contains this documentation and all open source infrastructure code</li> <li>https://github.com/application-research/estuary-hosted-infrastructure-private - Encrypted Git copy of relevant secrets and variables that cannot be shared publicly</li> <li>https://github.com/application-research/ehi-proxmaas - \"ProxMAAS\" repo, where all machine deployments are currently defined and a tool that we use to deploy machines automatically</li> </ul>"},{"location":"about/#ansible-repos","title":"Ansible repos","text":"<p>We love Ansible, and all our Ansible is open source. It's a little rough around the edges and could use some documentation love - PRs welcome!</p> <ul> <li>https://github.com/application-research/edge-playbook</li> <li>https://github.com/application-research/delta-playbook</li> <li>https://github.com/application-research/delta-dm-playbook</li> <li>https://github.com/application-research/edge-urid-playbook</li> <li>https://github.com/application-research/wildcard-tls-playbook</li> <li>https://github.com/application-research/haproxy-cluster-playbook</li> <li>https://github.com/application-research/postgresql-cluster-playbook</li> </ul>"},{"location":"fdi-backups-examine/","title":"Examine FDI Backups","text":""},{"location":"fdi-backups-examine/#check-garagehq-backups-for-k8s-dbs","title":"Check GarageHQ Backups for k8s DBs","text":"<ul> <li> <p>Login to Rancher</p> </li> <li> <p>Select 'Production Estuary' namespace in top right corner</p> </li> <li> <p>Go 'Workloads' &gt; 'Pods'</p> </li> <li> <p>Open shell in either estuary-api-0 or estuary-api-1 pods:</p> </li> </ul> <p>Enter this command to examine backups:</p> <p><code>envdir \"/run/etc/wal-e.d/env\" wal-g backup-list</code></p>"},{"location":"fdi-backups-examine/#examine-pgbackrest-backups-for-vm-dbs","title":"Examine pgBackRest Backups for VM DBs","text":""},{"location":"fdi-backups-examine/#list-pgbackrest-config-stanza-names","title":"List pgbackrest config + stanza names","text":"<p><code>ubuntu@prod-db-backup01:~$ sudo cat /etc/pgbackrest/pgbackrest.conf</code></p>"},{"location":"fdi-backups-examine/#check-backups-are-functional-for-stanza-fdi_main","title":"Check Backups are functional for stanza fdi_main","text":"<p><code>ubuntu@prod-db-backup01:~$ sudo -u pgbackrest pgbackrest --stanza=fdi_main --log-level-console=info check</code></p>"},{"location":"fdi-backups-examine/#check-backup-history-for-stanza-fdi_main","title":"Check backup history for stanza fdi_main","text":"<pre><code>ubuntu@prod-db-backup01:~$ sudo -u pgbackrest pgbackrest --stanza=fdi_main --log-level-console=info info\nstanza: fdi_main\n    status: ok\n    cipher: aes-256-cbc\n\n    db (current)\n        wal archive min/max (14): 0000010500000352000000DD/0000010900000362000000A4\n\n        full backup: 20230618-031405F\n            timestamp start/stop: 2023-06-18 03:14:05 / 2023-06-18 03:30:27\n            wal start/stop: 0000010500000352000000DD / 0000010500000352000000DD\n            database size: 211.9GB, database backup size: 211.9GB\n            repo1: backup set size: 61.7GB, backup size: 61.7GB\n\n        full backup: 20230625-031405F\n            timestamp start/stop: 2023-06-25 03:14:05 / 2023-06-25 03:29:58\n            wal start/stop: 000001080000035A000000C4 / 000001080000035A000000C4\n            database size: 213.1GB, database backup size: 213.1GB\n            repo1: backup set size: 62GB, backup size: 62GB\n</code></pre>"},{"location":"fdi-manual-checks/","title":"FDI Manual Checks","text":"<p>A long list of manual checks that can be done to examine all the important components in FDI.</p>"},{"location":"fdi-manual-checks/#bastion-servers","title":"Bastion Servers","text":"<p>pcadmin@workstation:~$ for i in 01 02 03; do ssh \"wings@prod-bastion${i}.estuary.tech\" -t \"sudo systemctl status netbird.service &amp;&amp; exit\"; done</p> <p>pcadmin@workstation:~$ for i in 01 02 03; do ssh \"wings@prod-bastion${i}.estuary.tech\" -t \"sudo journalctl -n 25 -u netbird.service &amp;&amp; exit\"; done</p>"},{"location":"fdi-manual-checks/#ceph","title":"Ceph","text":"<p>https://10.24.0.204:8006/#v1:0:=node%2Faltair:4:38::::::38</p> <p>Check that Ceph is: - OK - Has at least 3 Monitors, Managers and Meta Data Servers - Is not over 80% full - Logs</p>"},{"location":"fdi-manual-checks/#moosefs","title":"MooseFS","text":"<p>http://mfsmaster.estuary.tech:9425/mfs.cgi</p> <p>Check that the MooseFS:</p> <ul> <li>masters are up!</li> <li>no missing chunks</li> <li>no errors on any Disk</li> <li>chunk servers are OK</li> </ul>"},{"location":"fdi-manual-checks/#estuary-status-page","title":"Estuary Status Page","text":"<p>https://status.estuary.tech/</p>"},{"location":"fdi-manual-checks/#checkmk","title":"CheckMK","text":"<p>Examine:</p> <p>https://monitoring.estuary.tech/estuary/check_mk/login.py</p> <p>Green good, red bad! :)</p>"},{"location":"fdi-manual-checks/#database-clusters","title":"Database Clusters","text":"<p>$ ssh prod-ehi-db01.estuary.tech -t \"sudo patronictl -c /etc/patroni/prod-ehi-db01.estuary.tech.yml list\" + Cluster: prod_ehi_db ------+------------+---------+---------+----+-----------+-----------------+ | Member                     | Host       | Role    | State   | TL | Lag in MB | Pending restart | +----------------------------+------------+---------+---------+----+-----------+-----------------+ | prod-ehi-db01.estuary.tech | 10.24.3.20 | Leader  | running | 76 |           | *               | | prod-ehi-db02.estuary.tech | 10.24.3.21 | Replica | running | 76 |         0 | *               | | prod-ehi-db03.estuary.tech | 10.24.3.22 | Replica | running | 76 |         0 | *               | +----------------------------+------------+---------+---------+----+-----------+-----------------+</p> <p>$ ssh prod-ebi-db01.estuary.tech -t \"sudo patronictl -c /etc/patroni/prod-ebi-db01.estuary.tech.yml list\" + Cluster: prod_ebi_db ------+-----------+---------+---------+----+-----------+ | Member                     | Host      | Role    | State   | TL | Lag in MB | +----------------------------+-----------+---------+---------+----+-----------+ | prod-ebi-db01.estuary.tech | 10.24.3.1 | Leader  | running | 13 |           | | prod-ebi-db02.estuary.tech | 10.24.3.2 | Replica | running |    |        16 | | prod-ebi-db03.estuary.tech | 10.24.3.3 | Replica | running |    |        16 | +----------------------------+-----------+---------+---------+----+-----------+</p>"},{"location":"fdi-manual-checks/#examine-the-ram-usage","title":"Examine the RAM usage:","text":"<p>$ for i in 01 02 03; do ssh \"prod-ehi-db${i}.estuary.tech\" -t \"free -h\"; done</p> <p>$ for i in 01 02 03; do ssh \"prod-ebi-db${i}.estuary.tech\" -t \"free -h\"; done</p>"},{"location":"fdi-manual-checks/#examine-the-disk-space-usage","title":"Examine the disk space usage:","text":"<p>$ for i in 01 02 03; do ssh \"prod-ehi-db${i}.estuary.tech\" -t \"df -h\"; done</p> <p>$ for i in 01 02 03; do ssh \"prod-ebi-db${i}.estuary.tech\" -t \"df -h\"; done</p>"},{"location":"fdi-manual-checks/#haproxy","title":"HAProxy","text":"<p>$ for i in 01 02 03; do ssh \"prod-haproxy${i}.estuary.tech\" -t \"sudo crm_mon -r -1 &amp;&amp; exit\"; done</p> <p>$ for i in 01 02 03; do ssh \"prod-haproxy${i}.estuary.tech\" -t \"sudo pcs status &amp;&amp; exit\"; done</p> <p>$ for i in 01 02 03; do ssh \"prod-haproxy${i}.estuary.tech\" -t \"sudo systemctl status corosync.service &amp;&amp; exit\"; done</p>"},{"location":"fdi-manual-checks/#delta","title":"Delta","text":"<p>$ for i in 01 02 03 04 05 06 07 08; do ssh \"prod-ehi-delta${i}.estuary.tech\" -t \"sudo systemctl --no-pager status delta.service &amp;&amp; exit\"; done</p>"},{"location":"fdi-manual-checks/#edge","title":"Edge","text":"<p>$ for i in 01 02 03 04 05 06 07 08; do ssh \"prod-ehi-edge${i}.estuary.tech\" -t \"sudo systemctl --no-pager status edge.service &amp;&amp; exit\"; done</p>"},{"location":"fdi-manual-checks/#edge-urdi","title":"Edge-urdi","text":"<p>$ for i in 01 02 03 04 05 06 07 08; do ssh \"prod-ehi-edge-urid${i}.estuary.tech\" -t \"sudo systemctl --no-pager status edge.service &amp;&amp; exit\"; done</p>"},{"location":"getting-started/","title":"Getting started","text":"<p>Hello world</p>"},{"location":"restarting-ha-services/","title":"How to restart HA nodes/services?","text":"<p>All the services in FDI are designed to be highly-available, but still care must be taken when restarting certain machines/services.</p> <p>For most HA triplets, each node can just be updated/rebooted one by one.</p>"},{"location":"restarting-ha-services/#edge-and-delta","title":"Edge and Delta","text":"<p>The status of these services should be observed on your HAProxy's status page during these restarts. For example: https://prod-haproxy01.estuary.tech:8443/</p> <p></p> <p>These services are easy to reset, simply loop over them: <pre><code>$ for i in 01 02 03 04 05 06 07 08; do ssh \"prod-ehi-delta${i}.estuary.tech\" -t \"sudo apt update &amp;&amp; sudo apt upgrade -y &amp;&amp; sudo systemctl stop delta.service &amp;&amp; sudo reboot\"; done\n$ for i in 01 02 03 04 05 06 07 08; do ssh \"prod-ehi-edge${i}.estuary.tech\" -t \"sudo apt update &amp;&amp; sudo apt upgrade -y &amp;&amp; sudo systemctl stop edge.service &amp;&amp; sudo reboot\"; done\n$ for i in 01 02 03 04 05 06 07 08; do ssh \"prod-ehi-edge-urid${i}.estuary.tech\" -t \"sudo apt update &amp;&amp; sudo apt upgrade -y &amp;&amp; sudo systemctl stop edge.service &amp;&amp; sudo reboot\"; done\n</code></pre></p>"},{"location":"restarting-ha-services/#postgresqlpatroni-clusters","title":"Postgresql/Patroni clusters:","text":"<p>1) First use patronictl to observe the current leader: <pre><code>$ ssh prod-ehi-db01.estuary.tech -t \"sudo patronictl -c /etc/patroni/prod-ehi-db01.estuary.tech.yml list\"\n+ Cluster: prod_ehi_db ------+------------+---------+---------+-----+-----------+\n| Member                     | Host       | Role    | State   |  TL | Lag in MB |\n+----------------------------+------------+---------+---------+-----+-----------+\n| prod-ehi-db01.estuary.tech | 10.24.3.20 | Replica | running | 265 |         0 |\n| prod-ehi-db02.estuary.tech | 10.24.3.21 | Leader  | running | 265 |           |\n| prod-ehi-db03.estuary.tech | 10.24.3.22 | Replica | running | 265 |         0 |\n+----------------------------+------------+---------+---------+-----+-----------+\n</code></pre></p> <p>2) Patch one node at a time except for the leader, restarting as you go: <pre><code>$ ssh prod-ehi-db01.estuary.tech -t \"sudo apt update &amp;&amp; sudo apt upgrade -y &amp;&amp; sudo reboot\"\n$ ssh prod-ehi-db03.estuary.tech -t \"sudo apt update &amp;&amp; sudo apt upgrade -y &amp;&amp; sudo reboot\"\n</code></pre></p> <p>3) Examine if the other (Replica) nodes have come back online: <pre><code>$ ssh \"prod-ehi-db01.estuary.tech\" -t \"sudo patronictl -c /etc/patroni/prod-ehi-db01.estuary.tech.yml list\"\n+ Cluster: prod_ehi_db ------+------------+---------+---------+-----+-----------+\n| Member                     | Host       | Role    | State   |  TL | Lag in MB |\n+----------------------------+------------+---------+---------+-----+-----------+\n| prod-ehi-db01.estuary.tech | 10.24.3.20 | Replica | running | 265 |         0 |\n| prod-ehi-db02.estuary.tech | 10.24.3.21 | Leader  | running | 265 |           |\n| prod-ehi-db03.estuary.tech | 10.24.3.22 | Replica | running | 265 |         0 |\n+----------------------------+------------+---------+---------+-----+-----------+\n</code></pre></p> <p>4) Then do a planned switchover from leader to follower and finally patch the remaining machine: <pre><code>$ ssh prod-ehi-db01.estuary.tech -t \"sudo patronictl -c /etc/patroni/prod-ehi-db01.estuary.tech.yml switchover --candidate prod-ehi-db01.estuary.tech\"\nCurrent cluster topology\n+ Cluster: prod_ehi_db ------+------------+---------+---------+-----+-----------+\n| Member                     | Host       | Role    | State   |  TL | Lag in MB |\n+----------------------------+------------+---------+---------+-----+-----------+\n| prod-ehi-db01.estuary.tech | 10.24.3.20 | Replica | running | 265 |         0 |\n| prod-ehi-db02.estuary.tech | 10.24.3.21 | Leader  | running | 265 |           |\n| prod-ehi-db03.estuary.tech | 10.24.3.22 | Replica | running | 265 |         0 |\n+----------------------------+------------+---------+---------+-----+-----------+\nPrimary [prod-ehi-db02.estuary.tech]:            \nWhen should the switchover take place (e.g. 2023-06-29T13:23 )  [now]: \nAre you sure you want to switchover cluster prod_ehi_db, demoting current leader prod-ehi-db02.estuary.tech? [y/N]: y\n2023-06-29 12:27:32.40370 Successfully switched over to \"prod-ehi-db01.estuary.tech\"\n+ Cluster: prod_ehi_db ------+------------+---------+----------+-----+-----------+\n| Member                     | Host       | Role    | State    |  TL | Lag in MB |\n+----------------------------+------------+---------+----------+-----+-----------+\n| prod-ehi-db01.estuary.tech | 10.24.3.20 | Leader  | running  | 265 |           |\n| prod-ehi-db02.estuary.tech | 10.24.3.21 | Replica | stopping |     |   unknown |\n| prod-ehi-db03.estuary.tech | 10.24.3.22 | Replica | running  | 265 |         0 |\n+----------------------------+------------+---------+----------+-----+-----------+\n$ ssh prod-ehi-db02.estuary.tech -t \"sudo apt update &amp;&amp; sudo apt upgrade -y &amp;&amp; sudo reboot\"\n</code></pre></p> <p>5) Run patronictl list again to examine if all the nodes are still working.</p> <p>6) After completion also be sure to update the prod-backup-db01 server as it needs to have the exact same version of the pgBackRest package to function:</p> <p><code>$ ssh prod-db-backup01.estuary.tech -t \"sudo apt update &amp;&amp; sudo apt upgrade -y &amp;&amp; sudo reboot\"</code></p>"},{"location":"restarting-ha-services/#other-ha-services","title":"Other HA Services","text":"<p>The status of these services should be observed on your HAProxy's status page during these restarts. For example: https://prod-haproxy01.estuary.tech:8443/</p> <p></p> <pre><code>$ for i in 01 02 03; do ssh \"prod-fwscdn${i}.estuary.tech\" -t \"sudo apt update &amp;&amp; sudo apt upgrade -y &amp;&amp; sudo reboot\"; done\n$ for i in 01 02 03; do ssh \"prod-carrier${i}.estuary.tech\" -t \"sudo apt update &amp;&amp; sudo apt upgrade -y &amp;&amp; sudo reboot\"; done\n$ for i in 01 02 03 04 05; do ssh \"prod-nsqlookupd${i}.estuary.tech\" -t \"sudo apt update &amp;&amp; sudo apt upgrade -y &amp;&amp; sudo reboot\"; done\n</code></pre>"},{"location":"admin/basics/","title":"Basics","text":""},{"location":"admin/basics/#proxmox","title":"Proxmox","text":"<p>Proxmox is accessible at https://proxmox.estuary.tech:8006 using these credentials</p> <p>It provides a single-pane-of-glass view of everything that powers FDI, and should be considered the \"first port of call\" for most troubleshooting.</p> <p>Within Proxmox, you can stop, start and reboot VMs, as well as migrate them with zero functional downtime between physical hosts, and access the \"physical\" console of each virtual machine (in other words, attach a monitor, keyboard and mouse to a virtual machine and diagnose what's happening with it).</p> <p>Behind the scenes, https://proxmox.estuary.tech:8006 is powered by the physical node Vega. If Vega is down, you can access the other nodes - try https://10.24.0.202:8006, https://10.24.0.203:8006 etc.</p>"},{"location":"admin/basics/#dns","title":"DNS","text":"<p>Our DNS is powered by Technitium DNS, and the machines prod-dns01, prod-dns02, prod-dns03.</p> <p>These DNS servers are at 10.24.0.51, 10.24.0.52, 10.24.0.53 respectively.</p> <p>Netbird (which you'll usually use to stay connected to FDI) automatically sets your machine to use these DNS servers so you can access internal resources.</p>"},{"location":"admin/basics/#dhcp","title":"DHCP","text":"<p>Most core machines do not use DHCP for their addresses, but some do. DHCP is dependent on MAAS - Metal as a Service - which is an Estuary Bootstrap service. It runs primarily on prod-ebi-maas01.</p>"},{"location":"admin/basics/#booting-machines-normally","title":"Booting machines (normally)","text":"<p>To start a machine, use Proxmox to start it. If it's stuck, check the console and try and look for clues as to why it is not booting. If no machines are booting (or if many machines are not booting) check Ceph's health in the Proxmox interface and make sure it's healthy.</p>"},{"location":"admin/basics/#creating-deleting-and-managing-machines-and-machine-definitions","title":"Creating, deleting and managing machines and machine definitions","text":"<p>From time to time, it's necessary to create new virtual machines. You can do this easily using the ProxMAAS repo and Vega.</p> <ul> <li>Check out the ehi-proxmaas repo somewhere you can work on it.</li> <li>Within the ehi-proxmaas project, look under <code>vars/production-ehi</code> for the machine definitions used to spawn machines.</li> <li>Create a machine definition</li> <li>Run the spawn.yml ansible playbook against that machine definition</li> </ul> <p>For further details and the actual commands necessary, consult the ProxMAAS documentation. </p> <p>To spawn machines using ProxMAAS using Vega (one of the Proxmox nodes), ssh to root@vega.estuary.tech, and <code>cd ehi-proxmaas</code>. </p> <p>To set up a local machine to be able to spawn machines, make sure Netbird is installed, then copy <code>vars/secrets.yml</code> from <code>root@vega.estuary.tech:/root/ehi-proxmaas/vars/secrets.yml</code> to your local copy of the repository, making sure to avoid committing the secrets.</p>"},{"location":"admin/basics/#monitoring-and-logging","title":"Monitoring and Logging","text":"<p>There are a number of things that need monitoring throughout the infrastructure. We have various tools for this purpose. Here's a few of them:</p> <ul> <li> <p>Our centralised monitoring solution is provided by CheckMK Enterprise. A VM hosted on Vultr Cloud lives outside the infrastructure and monitors all of our hosts for issues, everything from dropped packets to disks filling and services not working. You can find the login for CheckMK in the password manager.</p> </li> <li> <p>The MooseFS CGI monitor allows us to keep an eye on MooseFS and should be checked frequently for warnings and error messages and dead disks and such. Efforts are underway to integrate information from the MooseFS API into CheckMK and open source those checks.</p> </li> <li> <p>The Ceph dashboard allows us to keep an eye on Ceph and its health, which should be tended to carefully as Ceph underpins all VMs and all workloads running on FDI</p> </li> <li> <p>ArgoCD provides a view into Customer Provisioned Infrastructure (Phosphophyllite), and can be used to see at a glance whether any customers' Kubernetes resources appear \"out of sync\" with what they should be, which could indicate problems.</p> </li> </ul>"},{"location":"admin/basics/#backups","title":"Backups","text":"<p>There are two core backup services that run in FDI at the moment.</p> <p>These are:</p> <ul> <li>pgBackRest @ prod-db-backup01 (for PostgreSQLs running as VMs)</li> <li>WAL-G @ Kubernetes [via GarageHQ] (for PostgreSQLs running as Kubernetes pods)</li> </ul>"},{"location":"admin/basics/#wal-g-kubernetes-via-garagehq","title":"WAL-G @ Kubernetes [via GarageHQ]","text":"<p>We run WAL-G to backup all PostgreSQL databases running in Kubernetes. It runs inside the <code>postgres</code> containers inside each PostgreSQL pod.</p> <p>It backs up to GarageHQ using the <code>postgresql-backups</code> key pair and the https://s3.estuary.tech endpoint.</p> <p>In order to check the status of backups for a particular cluster, find a pod within that cluster in Rancher, select the (\u22ee) and click <code>&gt; Execute Shell</code> to get a shell inside the container, then run the following command inside it: <pre><code>envdir \"/run/etc/wal-e.d/env\" wal-g backup-list\n</code></pre></p> <p>You should see something like this, with recent backups listed. <pre><code>root@estuary-api-0:/home/postgres# envdir \"/run/etc/wal-e.d/env\" wal-g backup-list\nname                          modified             wal_segment_backup_start\nbase_0000001A00001E510000000E 2023-07-06T11:29:59Z 0000001A00001E510000000E\nbase_0000001A00001E510000005C 2023-07-07T10:33:08Z 0000001A00001E510000005C\nbase_0000001B00001E51000000A0 2023-07-08T09:31:31Z 0000001B00001E51000000A0\nbase_0000001B00001E51000000D5 2023-07-09T10:26:17Z 0000001B00001E51000000D5\nbase_0000001B00001E520000000C 2023-07-10T11:20:18Z 0000001B00001E520000000C\nbase_0000001B00001E5200000044 2023-07-11T09:16:29Z 0000001B00001E5200000044\n</code></pre></p>"},{"location":"admin/basics/#pgbackrest","title":"pgBackRest","text":"<p>pgBackRest is currently set up to backup the EHI PostgreSQL cluster. It lives on <code>prod-db-backup01.estuary.tech</code>, and must remain functional. If the pgBackRest repo server (prod-db-backup01) is down, PostgreSQL has nowhere to archive WAL logs to and will collect them indefinitely, causing an eventual out-of-disk-space condition which will take down the EHI PostgreSQL cluster.</p> <p>It currently backs up to Ceph, and depends on an external Ceph backup for backups to remain 100% safe.</p> <p>That bears repeating!</p> <p>pgBackRest being down or non-functional means the EHI database - which EstuaryV2 depends on heavily - will eventually run out of space and stop.</p>"},{"location":"admin/basics/#checking-on-pgbackrest","title":"Checking on pgBackRest","text":"<p>There are two relevant commands for pgBackRest - <code>info</code> and <code>check</code>.</p> <p>Check ensures that WAL archiving is working and sane, which is critical to backups working and continuing to work. Here's how you run it: <pre><code>ubuntu@prod-db-backup01:~$ sudo -u pgbackrest pgbackrest --stanza=fdi_main --log-level-console=info check\n2023-07-12 17:04:38.783 P00   INFO: check command begin 2.46: --exec-id=128661-f4c4ef14 --log-level-console=info --pg1-host=prod-ehi-db01.estuary.tech --pg2-host=prod-ehi-db02.estuary.tech --pg3-host=prod-ehi-db03.estuary.tech --pg1-path=/var/lib/postgresql/14/prod_ehi_db/ --pg2-path=/var/lib/postgresql/14/prod_ehi_db/ --pg3-path=/var/lib/postgresql/14/prod_ehi_db/ --pg1-port=5432 --pg2-port=5432 --pg3-port=5432 --repo1-cipher-pass=&lt;redacted&gt; --repo1-cipher-type=aes-256-cbc --repo1-path=/var/lib/pgbackrest --stanza=fdi_main\n2023-07-12 17:04:42.393 P00   INFO: check repo1 (standby)\n2023-07-12 17:04:42.394 P00   INFO: switch wal not performed because this is a standby\n2023-07-12 17:04:42.394 P00   INFO: check repo1 configuration (primary)\n2023-07-12 17:04:42.596 P00   INFO: check repo1 archive for WAL (primary)\n2023-07-12 17:04:43.008 P00   INFO: WAL segment 000001110000052D000000CB successfully archived to '/var/lib/pgbackrest/archive/fdi_main/14-1/000001110000052D/000001110000052D000000CB-4285acb0ced7c37208bad415bbb53f82450b1aa0.gz' on repo1\n2023-07-12 17:04:43.309 P00   INFO: check command end: completed successfully (4528ms)\n</code></pre></p> <p>Info allows you to check the status of backups and ensure there are recent complete backups of PostgreSQL available in case of a disaster. Here's how you run it: <pre><code>ubuntu@prod-db-backup01:~$ sudo -u pgbackrest pgbackrest --stanza=fdi_main --log-level-console=info info\nstanza: fdi_main\n    status: ok\n    cipher: aes-256-cbc\n\n    db (current)\n        wal archive min/max (14): 0000010500000352000000DD/000001110000052D000000CC\n\n        full backup: 20230618-031405F\n            timestamp start/stop: 2023-06-18 03:14:05 / 2023-06-18 03:30:27\n            wal start/stop: 0000010500000352000000DD / 0000010500000352000000DD\n            database size: 211.9GB, database backup size: 211.9GB\n            repo1: backup set size: 61.7GB, backup size: 61.7GB\n\n        full backup: 20230625-031405F\n            timestamp start/stop: 2023-06-25 03:14:05 / 2023-06-25 03:29:58\n            wal start/stop: 000001080000035A000000C4 / 000001080000035A000000C4\n            database size: 213.1GB, database backup size: 213.1GB\n            repo1: backup set size: 62GB, backup size: 62GB\n</code></pre></p>"},{"location":"admin/basics/#security-patching","title":"Security &amp; Patching","text":""},{"location":"admin/basics/proxmaas/","title":"ProxMAAS","text":"<p>ProxMAAS allows for you to define virtual hosts in variable files and then create them automatically. New hosts will be created inside Proxmox, booted, then automatically installed and managed by MAAS.</p>"},{"location":"admin/basics/proxmaas/#creating-machine-definitions-for-spawning-vms","title":"Creating machine definitions (for spawning VMs)","text":"<p>1) In the ehi-proxmaas playbook, create a new machine definition (variable) file that defines the new hosts you want to make: <pre><code>user@workstation:~/projects/ehi-proxmaas$ cat vars/production-ehi/prod-garage.yml \n---\n# Machine specs\nscsi_disk_layout:\n  scsi0: \"vm-storage:50,format=raw\"\namount_of_memory: \"16384\"\nnumber_of_vcpus: \"64\"\nip_range:\n  - start: 10.24.100.1\n    end: 10.24.100.2\nstarting_number: \"1\"\nvm_name: staff-schreck\nvm_description: \"Schreck's personal machines\"\n</code></pre></p> <p>Here's a rough breakdown of what each of those do:</p> <ul> <li> <p>scsi_disk_layout: What disks the VM should have. scsi0 is always the root disk, vm-storage represents the Proxmox storage pool that will be used (always use vm-storage), 300 represents the number of gibibytes (GiB) assigned to that disk, and the rest are common formatting options which you should leave alone unless told otherwise. You can specify additional disks as scsi1, scsi2 etc in the same format.</p> </li> <li> <p>amount_of_memory: The amount of memory for the VM, specified in mebibytes (MiB). Should always be a multiple of 1024.</p> </li> <li> <p>number_of_vcpus: The amount of virtual CPUs assigned to the VM. This cannot exceed 96 due to physical limitations, but should usually never exceed 48, and should always be a multiple of 2.</p> </li> <li> <p>ip_range: This controls both what IP addresses are assigned, as well as how many machines will be spawned. You can define a single machine per definition, or define an IP range containing multiple machines. They will be named according to the number of total IPs + the starting_number you specify.</p> </li> <li> <p>starting_number: The starting number for the set of VMs. Setting this to \"1\" will result in machines being spawned as \"staff-schreck01, staff-schreck02 ...\", setting it to a higher number will result in the VMs starting at that higher number.</p> </li> <li> <p>vm_name: The name to assign to the VM in DNS, MAAS and Proxmox.</p> </li> <li> <p>vm_description: A useful description that will appear in Proxmox explaining what the VM is and does.</p> </li> </ul> <p>Once you have created and committed your machine definition...</p> <p>2) Run the spawn.yml playbook in ProxMAAS and define a {{ machine_details }} variable like so: <pre><code>user@workstation:~/projects/ehi-proxmaas$ ansible-playbook spawn.yml --extra-vars \"machine_details=prod-garage\"\n</code></pre></p>"},{"location":"admin/basics/proxmaas/#destroying-vms","title":"Destroying VMs","text":"<p>The process for destroying VMs is quite similar, but is interactive. Prior to destroying VMs, make sure they are powered off.</p> <pre><code>user@workstation:~/projects/ehi-proxmaas$ ansible-playbook destroy.yml\n...\n\nTASK [Set the VM name for the VMs] **************************************************************************************************************************************\n[Set the VM name for the VMs]\nEnter the VM name for the VMs you want to destroy. PERMANENTLY!:\nprod-garage\n...\nTASK [Set the starting number for the VMs] ******************************************************************************************************************************\n[Set the starting number for the VMs]\nEnter the starting number for the VMs (e.g. 01 if it's a new deployment, 04 if there are already 3 machines, etc.):\n01\n...\nTASK [Set the starting number for the VMs] ******************************************************************************************************************************\n[Set the starting number for the VMs]\nEnter the number of VMs you want to DESTROY:\n1\n...\nTASK [Ask the user if they wish to continue] ****************************************************************************************************************************\n[Ask the user if they wish to continue]\nAre you completely sure you wish to continue? (yesiamsure/n):\nyesiamsure\n</code></pre>"},{"location":"admin/databases/","title":"Databases","text":""},{"location":"admin/databases/#postgresql-on-kubernetes-phosphophyllite","title":"PostgreSQL on Kubernetes (Phosphophyllite)","text":"<p>We have various PostgreSQL clusters hosted on Phos k8s, including the most important one - <code>estuary-api</code>. Here's how to examine and manage them.</p>"},{"location":"admin/databases/#listing-postgresql-clusters","title":"Listing PostgreSQL clusters","text":"<p>Once you have <code>kubectl</code> configured and set up for access to <code>phosphophyllite</code>, you can access the Postgres Operator UI using kubectl port forwarding. This will give you a view of all PostgreSQL clusters running in Phosphophyllite, as well as the ability to spawn new PostgreSQL clusters if needed.</p> <p>You can read the quickstart guide for Postgres Operator for more information, but what you probably want is the following one liner:</p> <ul> <li><code>kubectl port-forward svc/postgres-operator-ui 8081:80 -n postgres</code></li> </ul> <p>Run that in a terminal (<code>screen</code> or <code>tmux</code> if you need it to run for a while) and in your favourite browser, navigate to http://localhost:8081 to open Postgres Operator UI.</p> <p></p> <p>Here be dragons...</p> <p>Postgres Operator UI will allow you to do lots of potential harm to customer and core PostgreSQL clusters in very short order with very little warning. Please be careful and try to know what you're doing - if you are not sure - ask!</p> <p>You will see all the PostgreSQL clusters we operate. You can click Status to check on the status of that particular cluster, Logs to view logs related to that cluster from the perspective of the Postgres Operator (as opposed to the service itself), Clone to create a new cluster from the latest available daily backup, Edit to edit the cluster or Delete to attempt to delete it.</p> <p>When 'Delete' doesn't mean Delete...</p> <p>Note that deleting a PostgreSQL cluster is not as easy as clicking that handy \"Delete\" button. There are delete protections in place that prevent accidental deletions of PostgreSQL clusters, as well as backups that will (eventually) need to be purged when deleting a cluster. For more, see \"Deleting clusters\" down below.</p>"},{"location":"admin/databases/#accessing-databases-how-do-i-access-a-database-cluster-on-kubernetes-to-check-on-its-status","title":"Accessing databases - how do I access a database cluster on Kubernetes to check on its status?","text":"<ul> <li>Open Rancher</li> <li>Go to the namespace containing the cluster you want to check on.</li> <li>Find one of the database pods (such as <code>dbfws-0</code>) for the database cluster in question, click the three dots (\u22ee) and click <code>&gt; Execute Shell</code> to get a shell inside the container.</li> <li>Run <code>patronictl list</code> and check that all replicas are on the same timeline (TL) and have no <code>Lag in MB.</code></li> </ul>"},{"location":"admin/databases/#accessing-databases-how-do-i-use-a-database-within-a-database-cluster","title":"Accessing databases - how do I use a database within a database cluster?","text":"<p>Access to databases requires <code>sslmode=prefer</code> or <code>sslmode=on</code>. <code>sslmode=disable</code> is not supported.</p> <ul> <li>Inside Kubernetes: You can access the database using the hostname <code>servicename.namespace</code> - such as <code>dbfws.hivemapper</code> - or just <code>servicename</code> if you are inside the same namespace as that service. Use the standard PostgreSQL port (5432) unless told otherwise.</li> <li>Outside Kubernetes: To access a database outside Kubernetes, we use MetalLB to expose a normal private IP address that can be accessed outside of Kubernetes.</li> </ul> <p>In order to access a database outside of Kubernetes, you will need the following settings in the <code>spec</code>: <pre><code>spec:\n  allowedSourceRanges:\n    - 10.24.0.0/16 # FDI main private IP space\n    - 10.42.0.0/16 # Internal Kubernetes private IP space\n  enableMasterLoadBalancer: true\n</code></pre></p>"},{"location":"admin/databases/#deleting-a-postgresql-cluster","title":"Deleting a PostgreSQL cluster","text":"<p>From time to time you may want or need to delete a database cluster. This may be as a result of a customer leaving or being terminated, or a cluster simply no longer being needed by either the customer or our team.</p> <p>Tread carefully!</p> <p>Deleting a PostgreSQL cluster is an irreversible action. Make sure you either have tested and working backups before you proceed, or are 100% sure you do not need the data.</p> <p>In order to delete a cluster, you must annotate the PostgreSQL CR with two annotations <code>delete-date</code> and <code>delete-clustername</code>. You can do this either through the Postgres Operator UI or through Rancher.</p> <p>Create an annotation called <code>delete-date</code> with the date in YYYY-MM-DD format, and an annotation called <code>delete-clustername</code> with the exact name of the cluster as defined in the CR.</p> <p>CPI is a special case</p> <p>CPI-deployed PostgreSQL clusters must first be \"orphaned\" via removal from <code>values.yaml</code> before they can be deleted in this way, and <code>dbfws</code> clusters (the default cluster included for each customer) CANNOT be deleted in this way at all. This is not exactly an intentional choice - Helm does not allow for outside modifications to its resources and will overwrite any changes you make to resources it manages. Thus, your annotations will be overwritten before they take effect.</p> <p>Once you have applied the annotations, you may delete the PostgreSQL cluster using any normal means (via Rancher, using <code>kubectl</code> or using the Operator UI).</p> <p>Deleting clusters does not cleanup or remove their backups. Note that you may cause undesired behaviour (restore from backup) if you create a new cluster with the same exact name and namespace as a previous cluster that was not cleaned up.</p>"},{"location":"admin/databases/#postgresql-vm-clusters","title":"PostgreSQL VM clusters","text":"<p>There are two main PostgreSQL clusters operating outside of Kubernetes. These are powered by a combination of <code>etcd</code>, <code>patroni</code>, <code>postgresql</code> and <code>haproxy</code>.</p>"},{"location":"admin/databases/#list-of-clusters","title":"List of clusters","text":"<ul> <li>Estuary Hosted Infrastructure (prod-ehi): Hosted on prod-ehi-db[01:03].estuary.tech</li> <li>Estuary Bootstrap Infrastructure (prod-ebi): Hosted on prod-ebi-db[01:03].estuary.tech</li> </ul>"},{"location":"admin/databases/#estuary-hosted-infrastructure-ehi-db","title":"Estuary Hosted Infrastructure (EHI) DB","text":"<p>The EHI database cluster runs database services for the following:</p> <ul> <li>EstuaryV2: Delta, Delta DM, Edge-UR, Edge-URID</li> <li>EstuaryV1: Shuttle 12, Edge Carriers,</li> <li>Gitea</li> </ul>"},{"location":"admin/databases/#estuary-bootstrap-infrastructure-ebi-db","title":"Estuary Bootstrap Infrastructure (EBI) DB","text":"<p>The EBI database cluster runs database services for the following:</p> <ul> <li>AWX automation</li> <li>Metal as a Service (MAAS) machine deployment</li> <li>Nautobot</li> </ul>"},{"location":"admin/databases/#accessing-databases-how-do-i-access-a-database-cluster-to-check-on-its-status","title":"Accessing databases - how do I access a database cluster to check on its status?","text":"<p>SSH into one of the servers that serve the database cluster. Here's an example using EHI:</p> <p><code>ssh ubuntu@prod-ehi-db01.estuary.tech</code></p> <p>Elevate to root:</p> <p><code>sudo su -</code></p> <p>Use <code>patronictl</code> to list the database members.</p> <pre><code>root@prod-ehi-db01:~# patronictl -c /etc/patroni/prod-ehi-db01.estuary.tech.yml list\n+ Cluster: prod_ehi_db ------+------------+---------+---------+-----+-----------+\n| Member                     | Host       | Role    | State   |  TL | Lag in MB |\n+----------------------------+------------+---------+---------+-----+-----------+\n| prod-ehi-db01.estuary.tech | 10.24.3.20 | Leader  | running | 273 |           |\n| prod-ehi-db02.estuary.tech | 10.24.3.21 | Replica | running | 273 |         0 |\n| prod-ehi-db03.estuary.tech | 10.24.3.22 | Replica | running | 273 |         0 |\n+----------------------------+------------+---------+---------+-----+-----------+\n</code></pre>"},{"location":"admin/databases/#accessing-databases-how-do-i-use-a-database-in-a-vm-cluster","title":"Accessing databases - how do I use a database in a VM cluster?","text":"<p>You can access the databases in the VM clusters using <code>postgres-ehi.estuary.tech</code> and <code>postgres-ebi.estuary.tech</code> (both are CNAMEs which point at the production HAProxies).</p> <p>The chosen TCP port is what determines which PostgreSQL cluster you connect to - 51432 for EBI, 52432 for EHI.</p>"},{"location":"admin/databases/estuary-postgres-quickstart/","title":"Quick Start Guide","text":"<p>1) Install the Ansible requirements:</p> <p><code>$ ansible-galaxy install -r requirements.yml</code></p> <p>2) This playbook requires a 'tls_bastion' server to be defined in the inventory, this is a server you're using exclusively for certificates/challenges etc. Using localhost is also an option if you don't have a tls_bastion server: <pre><code># add tls_bastion as localhost\n[tls_bastion]\nlocalhost ansible_connection=local\n</code></pre></p> <p>3) You must define a seperate inventory group for the {{ postgresql_cluster_name }} variable, for example if it's: <code>postgresql_cluster_name: prod_estuary</code></p> <p>Means you'll need to configure a group for it in your inventory/hosts file like so: <pre><code>[prod_estuary:children]\netcd_master\n</code></pre></p> <p>4) Define these extra variables in your ./inventories/pchq/group_vars/all.yml file: <pre><code>internal_subnet: \"10.1.1.0/16\"                 # The internal subnet of your network\npostgresql_cluster_name: prod_estuary        \npatroni_postgresql_version: 15\npatroni_install_haproxy: false\npatroni_system_group: postgres\npatroni_install_watchdog_loader: false         # This is needed as it disables a currently broken section\npatroni_etcd_hosts: \"{% for host in groups[postgresql_cluster_name] %}{{ hostvars[host]['inventory_hostname'] }}:2379{% if not loop.last %},{% endif %}{% endfor %}\"  # yamllint disable-line rule:line-length\npatroni_etcd_protocol: https\npatroni_etcd_cacert: \"/var/lib/patroni.pki/ca.pem\"\npatroni_etcd_cert: \"/var/lib/patroni.pki/{{ inventory_hostname }}.pem\"\npatroni_etcd_key: \"/var/lib/patroni.pki/{{ inventory_hostname }}-key.pem\"\npatroni_postgresql_connect_address: \"{{ ansible_ens18.ipv4.address }}:5432\"\npatroni_restapi_connect_address: \"{{ ansible_ens18.ipv4.address }}:{{ patroni_restapi_port }}\"\n\npatroni_postgresql_pg_hba:                                              # This section defines connection permissions between the postgresql hosts\n  - { type: \"local\", database: \"all\", user: \"all\", method: \"peer\" }\n  #- { type: \"host\", database: \"all\", user: \"postgres\", address: \"{{ internal_subnet }}\", method: \"scram-sha-256\" }\n  - { type: \"host\", database: \"replication\", user: \"{{ patroni_replication_username }}\", address: \"{{ internal_subnet }}\", method: \"scram-sha-256\" } # Allow Patroni replication\n  - { type: \"host\", database: \"replication\", user: \"{{ patroni_replication_username }}\", address: \"127.0.0.1/32\", method: \"scram-sha-256\" } # Allow local Patroni access\n</code></pre></p> <p>5) Run the playbook against your chosen inventory: <code>$ ansible-playbook -v -i ./inventories/prod site.yml</code></p> <p>or with an escalation password if needed: <code>$ ansible-playbook -i ./inventories/prod site.yml --ask-become-pass</code></p>"},{"location":"admin/databases/postgres-patroni-troubleshooting/","title":"Troubleshooting Guide","text":"<p>Note that the certificates this script generates MUST have the IP as a 'Subject Alternative Name', test the pubkey at this URL to verify it: https://www.sslshopper.com/certificate-decoder.html</p> <p>Once all that is done, you should have working PostgreSQL via the loadbalancer: <pre><code>root@prod-pg02:~# patronictl -c /etc/patroni/prod-pg02.estuary.tech.yml list\n+ Cluster: main -----------+-------------+---------+---------+----+-----------+\n| Member                   | Host        | Role    | State   | TL | Lag in MB |\n+--------------------------+-------------+---------+---------+----+-----------+\n| prod-pg01.estuary.tech | 10.1.11.119 | Replica | running |  1 |         0 |\n| prod-pg02.estuary.tech | 10.1.11.118 | Leader  | running |  1 |           |\n| prod-pg03.estuary.tech | 10.1.11.117 | Replica | running |  1 |         0 |\n+--------------------------+-------------+---------+---------+----+-----------+\n</code></pre></p> <p>Check if you can log into the postgresql database. <pre><code>root@estuary-pg01:~# psql -h 10.1.3.1 -U postgres -W\nPassword:\npsql (14.5 (Debian 14.5-2.pgdg110+2))\nType \"help\" for help.\n\npostgres=#\n</code></pre></p>"},{"location":"admin/databases/postgres-patroni-troubleshooting/#stats-page","title":"Stats Page","text":"<p>If you're using this playbook with HAProxy you can visit the loadbalancer's IP on port 8080 to see a stats page:</p> <p>http://prod-haproxy01.estuary.tech:8080/</p> <p>This stats page shows you which node is the current leader (shown in green).</p>"},{"location":"admin/databases/postgres-patroni-troubleshooting/#tls-secrets","title":"TLS Secrets","text":"<p>Keep a backup of the certificate material from the secure box, and (if needed) shut down and destroy the secure box once the material has been safely backed up and verified.</p>"},{"location":"admin/databases/re-init_database/","title":"Re-init a Postgresql Node","text":""},{"location":"admin/databases/re-init_database/#sometimes-a-postgresql-node-will-fall-out-of-sync-like-so","title":"Sometimes a postgresql node will fall out of sync like so:","text":"<pre><code>ubuntu@prod-ehi-db01:~$ sudo /usr/local/bin/patronictl -c /etc/patroni/prod-ehi-db01.estuary.tech.yml list\n+ Cluster: prod_ehi_db ------+------------+---------+--------------+-----+-----------+\n| Member                     | Host       | Role    | State        |  TL | Lag in MB |\n+----------------------------+------------+---------+--------------+-----+-----------+\n| prod-ehi-db01.estuary.tech | 10.24.3.20 | Replica | running      | 266 |    349570 |\n| prod-ehi-db02.estuary.tech | 10.24.3.21 | Replica | start failed |     |   unknown |\n| prod-ehi-db03.estuary.tech | 10.24.3.22 | Leader  | running      | 269 |           |\n+----------------------------+------------+---------+--------------+-----+-----------+\n</code></pre>"},{"location":"admin/databases/re-init_database/#the-failing-node-must-be-re-initialized","title":"The failing node must be re-initialized:","text":"<pre><code>ubuntu@prod-ehi-db01:~$ sudo /usr/local/bin/patronictl -c /etc/patroni/prod-ehi-db01.estuary.tech.yml reinit prod_ehi_db\n+ Cluster: prod_ehi_db ------+------------+---------+--------------+-----+-----------+\n| Member                     | Host       | Role    | State        |  TL | Lag in MB |\n+----------------------------+------------+---------+--------------+-----+-----------+\n| prod-ehi-db01.estuary.tech | 10.24.3.20 | Replica | running      | 266 |    349575 |\n| prod-ehi-db02.estuary.tech | 10.24.3.21 | Replica | start failed |     |   unknown |\n| prod-ehi-db03.estuary.tech | 10.24.3.22 | Leader  | running      | 269 |           |\n+----------------------------+------------+---------+--------------+-----+-----------+\nWhich member do you want to reinitialize [prod-ehi-db02.estuary.tech, prod-ehi-db01.estuary.tech]? []: prod-ehi-db02.estuary.tech\nAre you sure you want to reinitialize members prod-ehi-db02.estuary.tech? [y/N]: y\nSuccess: reinitialize for member prod-ehi-db02.estuary.tech\n</code></pre>"},{"location":"admin/databases/re-init_database/#this-changes-the-state-to-creating-replica","title":"This changes the state to 'creating replica':","text":"<pre><code>pcadmin@workstation:~$ ssh prod-ehi-db01.estuary.tech -t \"sudo patronictl -c /etc/patroni/prod-ehi-db01.estuary.tech.yml list\"\n+ Cluster: prod_ehi_db ------+------------+---------+------------------+-----+-----------+\n| Member                     | Host       | Role    | State            |  TL | Lag in MB |\n+----------------------------+------------+---------+------------------+-----+-----------+\n| prod-ehi-db01.estuary.tech | 10.24.3.20 | Replica | creating replica |     |   unknown |\n| prod-ehi-db02.estuary.tech | 10.24.3.21 | Replica | running          | 269 |         0 |\n| prod-ehi-db03.estuary.tech | 10.24.3.22 | Leader  | running          | 269 |           |\n+----------------------------+------------+---------+------------------+-----+-----------+\nConnection to prod-ehi-db01.estuary.tech closed.\n</code></pre>"},{"location":"admin/databases/update-customers-k8s-postgres/","title":"Update customers k8s postgres","text":""},{"location":"admin/databases/update-customers-k8s-postgres/#update-customers-postgresedgedelta","title":"Update customers postgres/edge/delta","text":"<p>It's hard to see in Rancher but there's logs indicating DB failure...</p> <p>https://rancher.estuary.tech/dashboard/c/c-m-xvjqhpz7/explorer/pod</p> <p>Apparently we need to update that customers postgres and edge/delta versions...</p> <p>1) https://git.estuary.tech/fws-customers/customer-est-ana/src/branch/main/kubernetes/Chart.yaml edit this file, but locally, in your own git checkout of it(edited)</p> <p>2) bump it to the latest postgresql and latest edge, look here if you need a reference for what the latest is: https://github.com/application-research/fws-cpi-helm (cpi-edge-0.9.5.tgz tells you Edge's latest is 0.9.5, and latest postgresql is 0.6.0)</p> <p>3) so, edit the repo, changing Chart.yaml to have the newer versions bump \"version\" on the chart itself as appropriate - if any of the charts changed a Y value in vX.Y.Z (ie: did more than a patch), bump the Y value on the chart so ana's chart should go from 0.1.1 to 0.2.0</p> <p>4) then before you commit it, cd to the kubernetes folder in the repo and run helm dependency update:</p> <pre><code>~/customer-est-ana/kubernetes$ helm dependency update\nGetting updates for unmanaged Helm repositories...\n...Successfully got an update from the \"https://application-research.github.io/fws-cpi-helm\" chart repository\n...Successfully got an update from the \"https://application-research.github.io/fws-cpi-helm\" chart repository\nSaving 2 charts\nDownloading cpi-edge from repo https://application-research.github.io/fws-cpi-helm\nDownloading cpi-postgresql from repo https://application-research.github.io/fws-cpi-helm\nDeleting outdated charts\n</code></pre> <p>5) then commit all changes, and wait for ArgoCD to roll it out</p> <p>pcadmin@workstation:~/MyVault/Estuary_LIVE/customer-est-ana$ git add . pcadmin@workstation:~/MyVault/Estuary_LIVE/customer-est-ana$ git commit -am'update versions of edge/postgres' [main 5a1d86a] update versions of edge/postgres  7 files changed, 10 insertions(+), 7 deletions(-)  create mode 100644 .vscode/settings.json  delete mode 100644 kubernetes/charts/cpi-edge-0.8.1.tgz  create mode 100644 kubernetes/charts/cpi-edge-0.9.5.tgz  delete mode 100644 kubernetes/charts/cpi-postgresql-0.2.2.tgz  create mode 100644 kubernetes/charts/cpi-postgresql-0.6.0.tgz pcadmin@workstation:~/MyVault/Estuary_LIVE/customer-est-ana$ git push</p>"},{"location":"admin/guides/","title":"Index","text":""},{"location":"admin/guides/#general-and-specialised-guides","title":"General and Specialised Guides","text":"<p>These are guides that don't (yet) fit in elsewhere in the documentation.</p> <ul> <li>Diagnosing Pacemaker clusters including Prod HAProxy</li> <li>Ceph: Rebalancing Ceph OSDs</li> <li>Kubernetes: Fixing stuck Rook.io volumes which are preventing containers from starting</li> <li>Restarting HA services for server updates</li> <li>Examine backups</li> <li>Manual checks for critical services</li> </ul>"},{"location":"admin/guides/01-diagnosing-pacemaker-clusters/","title":"Diagnosing Pacemaker clusters","text":"<p>We run a number of Pacemaker clusters, but the primary one to keep an eye on is the prod-haproxy cluster.</p> <p>This document is written about that cluster, but learnings from this one can be applied to other Pacemaker clusters (and to a lesser extent Proxmox, which uses Corosync, the same library Pacemaker uses, to keep its cluster members in sync and in quorum).</p>"},{"location":"admin/guides/01-diagnosing-pacemaker-clusters/#pacemakers-role-in-fdi","title":"Pacemaker's role in FDI","text":"<p>Pacemaker moves the virtual IP addresses 10.24.0.[2,3,4] around as needed, across the three Production HAProxy hosts. This allows us to use haproxy.estuary.tech to refer to the virtual IP addresses, and when a host fails or needs to be taken down for maintenance, the remaining two HAProxy hosts take over any IPs that were served by that third node and continue service like nothing happened.</p> <p>The Production HAProxy boxes run an open source load balancer (HAProxy) that allows proxying traffic of many kinds, everything from S3 gateways to PostgreSQL traffic and Kubernetes. Most of the lifeblood of Estuary and FDT runs through the HAProxies. The multiple virtual IP addresses also serve the purpose of load balancing traffic across the HAProxies, splitting it roughly evenly using round-robin DNS.</p>"},{"location":"admin/guides/01-diagnosing-pacemaker-clusters/#checking-the-status-of-the-cluster","title":"Checking the status of the cluster","text":"<p>To check the status of the Pacemaker cluster, use <code>sudo pcs status</code> on a haproxy node.</p> <pre><code>ubuntu@prod-haproxy01:~$ sudo pcs status\nCluster name: loadbalancer\nCluster Summary:\n  * Stack: corosync\n  * Current DC: prod-haproxy02.estuary.tech (version 2.1.2-ada5c3b36e2) - partition with quorum\n  * Last updated: Wed Jul 12 10:59:45 2023\n  * Last change:  Mon Jul 10 23:33:06 2023 by hacluster via crmd on prod-haproxy02.estuary.tech\n  * 3 nodes configured\n  * 6 resource instances configured\n\nNode List:\n  * Online: [ prod-haproxy01.estuary.tech prod-haproxy02.estuary.tech prod-haproxy03.estuary.tech ]\n\nFull List of Resources:\n  * virtual_ip_1    (ocf:heartbeat:IPaddr2):     Started prod-haproxy02.estuary.tech\n  * virtual_ip_2    (ocf:heartbeat:IPaddr2):     Started prod-haproxy03.estuary.tech\n  * virtual_ip_3    (ocf:heartbeat:IPaddr2):     Started prod-haproxy01.estuary.tech\n  * Clone Set: loadbalancer-clone [loadbalancer]:\n    * Started: [ prod-haproxy01.estuary.tech prod-haproxy02.estuary.tech prod-haproxy03.estuary.tech ]\n\nDaemon Status:\n  corosync: active/enabled\n  pacemaker: active/enabled\n  pcsd: active/enabled\n</code></pre> <p>You'll want to see that all 3 virtual_ip resources are being served by one node each, and that loadbalancer-clone is running on any nodes that are alive. This command will also tell you which nodes are up or down or in maintenance and allow you to take action accordingly.</p>"},{"location":"admin/guides/01-diagnosing-pacemaker-clusters/#debugging-when-things-go-wrong","title":"Debugging when things go wrong","text":"<p>If there are resources which have failed to stop or start in the cluster:</p> <ul> <li><code>sudo pcs resource cleanup</code></li> </ul> <p>If a node is out of sync with the rest of the cluster</p> <ul> <li><code>sudo systemctl restart corosync &amp;&amp; sudo systemctl restart pcsd</code></li> </ul> <p>Standard Pacemaker debugging applies - try to get at least two nodes to appear and be healthy so that resources can be served.</p>"},{"location":"admin/guides/02-rebalancing-ceph-osds/","title":"Rebalancing Ceph OSDs","text":"<p>From time to time, Ceph may get out of balance. This will look like various pools warning that they are full or near-full, and in particular OSDs reporting a full or near-full status.</p> <p>This is meant to be taken care of largely automatically by Ceph's <code>balancer</code> module for the ceph <code>mgr</code>, however in practice one may still need to intervene from time to time (as we had to a week or so before this guide was written, in late June 2023).</p> <p>The key command to know is this:</p> <p><code>ceph osd reweight-by-utilization 120 0.4 12 --no-increasing</code></p> <p>and a slightly more drastic version:</p> <p><code>ceph osd reweight-by-utilization 120 0.4 12</code></p> <p>In the command <code>ceph osd reweight-by-utilization 120 0.4 12 --no-increasing</code>:</p> <pre><code>120: This is the threshold of over-utilization percentage. If an OSD's usage is above 120% of the average utilization, the command will try to decrease its weight, therefore less data will be placed on it. The percentage is calculated based on the total usage of all OSDs in the system.\n\n0.4: This is the maximum change that can be made to an OSD's weight in a single invocation of the command. Here, no single OSD's weight will be changed by more than 40%.\n\n12: This is the max number of OSDs that can be changed in one command execution. The command will adjust the weights of up to 12 OSDs in a single run.\n\n--no-increasing: This option indicates that the command should only decrease the weight of OSDs, not increase. This is useful when you want to ensure that no OSD gets more data than it currently has, which might be important when some OSDs are close to their storage limit.\n</code></pre> <p>So, in summary, the command will reduce the weights of up to 12 most utilized OSDs (those with utilization over 120% of the average) in the cluster by a maximum of 40%, without increasing the weight of any OSDs. The weight reduction makes the data placement algorithm favor other OSDs for storing new data, which in turn should lower the utilization of the over-utilized OSDs over time.</p>"},{"location":"admin/guides/03-stuck-rook-volumes/","title":"Fixing stuck Rook volumes","text":"<p>For reasons unknown, occasionally Rook will get confused and leave a PersistentVolume falsely mounted on a node. This will cause issues the next time a pod is supposed to be scheduled that uses that PersistentVolume.</p> <p>You can check for containers affected by this by going to Workloads -&gt; Pods while \"All Namespaces\" is selected in the namespace selector (at the top right of Rancher).</p> <p>They will look like this - blue or red in a pending or Containercreating state, that never changes.</p> <p></p> <p>Here is a picture of an affected pod:</p> <p></p>"},{"location":"admin/guides/03-stuck-rook-volumes/#finding-the-affected-persistentvolume","title":"Finding the affected PersistentVolume","text":"<p>If you're lucky (as there is sometimes a bug that prevents this from displaying in Rancher), you will be able to select \"Recent Events\" and be told which PersistentVolume is preventing the pod from successfully starting.</p> <p>This is not the case in this screenshot, where none of the needed information is displayed: </p> <p>You can sometimes cause the name of the PersistentVolume to display by deleting and recreating the pod in question. You can do so by selecting the pod and clicking <code>Delete</code> - a Kubernetes operator such as ArgoCD will take care of respawning the pod (unless it's a one-time deployment)</p> <p>If after restarting the pod, you cannot view its information, use <code>kubectl describe</code> on the pod in question. Here's an example with the pod that was pictured earlier:</p> <p><code>$ kubectl describe pod dbfws-1 -n customer-ff</code></p> <p>You're looking for the <code>events</code> down the bottom, kind of like this:</p> <pre><code>Events:\n  Type     Reason              Age   From                     Message\n  ----     ------              ----  ----                     -------\n  Normal   Scheduled           2m7s  default-scheduler        Successfully assigned customer-ff/dbfws-1 to prod-phos-k8s-w26\n  Warning  FailedAttachVolume  2m7s  attachdetach-controller  Multi-Attach error for volume \"pvc-1b4b4d14-00f0-4af6-a1fb-4be1aba9de4d\" Volume is already exclusively attached to one node and can't be attached to another\n  Warning  FailedMount         4s    kubelet                  Unable to attach or mount volumes: unmounted volumes=[pgdata], unattached volumes=[pgdata dshm kube-api-access-749bz]: timed out waiting for the condition\n</code></pre> <p>This is what shows up when you've found the volume name successfully: </p> <p>We've identified now that the volume's ID starts with 1b4b4d14, which we can use to find where the PV is attached and un-attach it.</p>"},{"location":"admin/guides/03-stuck-rook-volumes/#finding-the-associated-volumeattachment","title":"Finding the associated VolumeAttachment","text":"<p>In Rancher, go to <code>More Resources -&gt; Storage -&gt; VolumeAttachments</code> and open it.</p> <p>Search for the volume's ID on the right, in this case \"1b4b4d14\".</p> <p></p> <p>Here we can see that the volume is attached on node <code>prod-phos-k8s-wxl02</code> in this case. Take a note of which node your volume is attached to.</p> <p>Leave this open as a tab for later.</p>"},{"location":"admin/guides/03-stuck-rook-volumes/#opening-a-shell-on-the-affected-node","title":"Opening a shell on the affected node","text":"<p>In a new tab, go back to <code>Workloads -&gt; Pods</code> and clear any filters you have set on namespaces or in the <code>Filter</code> box.</p> <p>Search for the name of your node using the <code>Filter</code> box to view only pods running on that node.</p> <p>Look for <code>csi-rbdplugin-SOMESTRING</code> (NB: do not use the one with \"provisioner\" in the name), then using the three dots select \"&gt; Execute Shell\" for that pod.</p> <p>In the dropdown in the bottom left, select <code>csi-rbdplugin</code> to get a shell into the right container.</p>"},{"location":"admin/guides/03-stuck-rook-volumes/#finding-which-device-id-the-volume-is-mounted-as","title":"Finding which device ID the volume is mounted as","text":"<p>Once in the shell, run <code>df</code> and grep for the name of the volume's ID as determined earlier:</p> <pre><code>[root@prod-phos-k8s-wxl02 /]# df | grep 1b4b4d14\n/dev/rbd6        32716560     264488   32435688   1% /var/lib/kubelet/plugins/kubernetes.io/csi/pv/pvc-1b4b4d14-00f0-4af6-a1fb-4be1aba9de4d/globalmount/0001-0012-rook-ceph-external-0000000000000008-68514ead-c3b0-4cc0-baa6-56428107ab61\n</code></pre> <p>There should be only one device that is mounted at that path, and it should be a <code>/dev/rbd</code> device. Take note of the number involved and proceed.</p>"},{"location":"admin/guides/03-stuck-rook-volumes/#cleaning-up-the-resource","title":"Cleaning up the resource","text":"<p>Now, roughly at the same time, go back to your tab from earlier with the VolumeAttachment. Delete the volumeattachment, and then in the <code>csi-rbdplugin</code> shell, type the following, using the name of the device as found in <code>df</code> (<code>/dev/rbd6 in this example</code>)</p> <pre><code>[root@prod-phos-k8s-wxl02 /]# rbd unmap -o force /dev/rbd6\n</code></pre> <p>Finally, delete the original pod that was having issues and let it naturally re-create.</p>"},{"location":"admin/guides/04-restarting-ha-services/","title":"How to restart HA nodes/services?","text":"<p>All the services in FDI are designed to be highly-available, but still care must be taken when restarting certain machines/services.</p> <p>For most HA triplets, each node can just be updated/rebooted one by one.</p>"},{"location":"admin/guides/04-restarting-ha-services/#edge-and-delta","title":"Edge and Delta","text":"<p>The status of these services should be observed on your HAProxy's status page during these restarts. For example: https://prod-haproxy01.estuary.tech:8443/</p> <p></p> <p>These services are easy to reset, simply loop over them: <pre><code>$ for i in 01 02 03 04 05 06 07 08; do ssh \"prod-ehi-delta${i}.estuary.tech\" -t \"sudo apt update &amp;&amp; sudo apt upgrade -y &amp;&amp; sudo reboot\"; done\n$ for i in 01 02 03 04 05 06 07 08; do ssh \"prod-ehi-edge${i}.estuary.tech\" -t \"sudo apt update &amp;&amp; sudo apt upgrade -y &amp;&amp; sudo reboot\"; done\n</code></pre></p>"},{"location":"admin/guides/04-restarting-ha-services/#postgresqlpatroni-clusters","title":"Postgresql/Patroni clusters:","text":"<p>1) First use patronictl to observe the current leader: <pre><code>$ ssh prod-ehi-db01.estuary.tech -t \"sudo patronictl -c /etc/patroni/prod-ehi-db01.estuary.tech.yml list\"\n+ Cluster: prod_ehi_db ------+------------+---------+---------+-----+-----------+\n| Member                     | Host       | Role    | State   |  TL | Lag in MB |\n+----------------------------+------------+---------+---------+-----+-----------+\n| prod-ehi-db01.estuary.tech | 10.24.3.20 | Replica | running | 265 |         0 |\n| prod-ehi-db02.estuary.tech | 10.24.3.21 | Leader  | running | 265 |           |\n| prod-ehi-db03.estuary.tech | 10.24.3.22 | Replica | running | 265 |         0 |\n+----------------------------+------------+---------+---------+-----+-----------+\n</code></pre></p> <p>2) Patch one node at a time except for the leader, restarting as you go: <pre><code>$ ssh prod-ehi-db01.estuary.tech -t \"sudo apt update &amp;&amp; sudo apt upgrade -y &amp;&amp; sudo reboot\"\n$ ssh prod-ehi-db03.estuary.tech -t \"sudo apt update &amp;&amp; sudo apt upgrade -y &amp;&amp; sudo reboot\"\n</code></pre></p> <p>3) Examine if the other (Replica) nodes have come back online: <pre><code>$ ssh \"prod-ehi-db01.estuary.tech\" -t \"sudo patronictl -c /etc/patroni/prod-ehi-db01.estuary.tech.yml list\"\n+ Cluster: prod_ehi_db ------+------------+---------+---------+-----+-----------+\n| Member                     | Host       | Role    | State   |  TL | Lag in MB |\n+----------------------------+------------+---------+---------+-----+-----------+\n| prod-ehi-db01.estuary.tech | 10.24.3.20 | Replica | running | 265 |         0 |\n| prod-ehi-db02.estuary.tech | 10.24.3.21 | Leader  | running | 265 |           |\n| prod-ehi-db03.estuary.tech | 10.24.3.22 | Replica | running | 265 |         0 |\n+----------------------------+------------+---------+---------+-----+-----------+\n</code></pre></p> <p>4) Then do a planned switchover from leader to follower and finally patch the remaining machine: <pre><code>$ ssh prod-ehi-db01.estuary.tech -t \"sudo patronictl -c /etc/patroni/prod-ehi-db01.estuary.tech.yml switchover --candidate prod-ehi-db01.estuary.tech\"\nCurrent cluster topology\n+ Cluster: prod_ehi_db ------+------------+---------+---------+-----+-----------+\n| Member                     | Host       | Role    | State   |  TL | Lag in MB |\n+----------------------------+------------+---------+---------+-----+-----------+\n| prod-ehi-db01.estuary.tech | 10.24.3.20 | Replica | running | 265 |         0 |\n| prod-ehi-db02.estuary.tech | 10.24.3.21 | Leader  | running | 265 |           |\n| prod-ehi-db03.estuary.tech | 10.24.3.22 | Replica | running | 265 |         0 |\n+----------------------------+------------+---------+---------+-----+-----------+\nPrimary [prod-ehi-db02.estuary.tech]:            \nWhen should the switchover take place (e.g. 2023-06-29T13:23 )  [now]: \nAre you sure you want to switchover cluster prod_ehi_db, demoting current leader prod-ehi-db02.estuary.tech? [y/N]: y\n2023-06-29 12:27:32.40370 Successfully switched over to \"prod-ehi-db01.estuary.tech\"\n+ Cluster: prod_ehi_db ------+------------+---------+----------+-----+-----------+\n| Member                     | Host       | Role    | State    |  TL | Lag in MB |\n+----------------------------+------------+---------+----------+-----+-----------+\n| prod-ehi-db01.estuary.tech | 10.24.3.20 | Leader  | running  | 265 |           |\n| prod-ehi-db02.estuary.tech | 10.24.3.21 | Replica | stopping |     |   unknown |\n| prod-ehi-db03.estuary.tech | 10.24.3.22 | Replica | running  | 265 |         0 |\n+----------------------------+------------+---------+----------+-----+-----------+\n$ ssh prod-ehi-db02.estuary.tech -t \"sudo apt update &amp;&amp; sudo apt upgrade -y &amp;&amp; sudo reboot\"\n</code></pre></p> <p>5) Run patronictl list again to examine if all the nodes are still working.</p> <p>6) After completion also be sure to update the prod-backup-db01 server as it needs to have the exact same version of the pgBackRest package to function:</p> <p><code>$ ssh prod-db-backup01.estuary.tech -t \"sudo apt update &amp;&amp; sudo apt upgrade -y &amp;&amp; sudo reboot\"</code></p>"},{"location":"admin/guides/04-restarting-ha-services/#other-ha-services","title":"Other HA Services","text":"<p>The status of these services should be observed on your HAProxy's status page during these restarts. For example: https://prod-haproxy01.estuary.tech:8443/</p> <p></p> <pre><code>$ for i in 01 02 03; do ssh \"prod-fwscdn${i}.estuary.tech\" -t \"sudo apt update &amp;&amp; sudo apt upgrade -y &amp;&amp; sudo reboot\"; done\n$ for i in 01 02 03; do ssh \"prod-carrier${i}.estuary.tech\" -t \"sudo apt update &amp;&amp; sudo apt upgrade -y &amp;&amp; sudo reboot\"; done\n</code></pre>"},{"location":"admin/guides/05-backups-examine/","title":"Examine FDI Backups","text":""},{"location":"admin/guides/05-backups-examine/#check-garagehq-backups-for-k8s-dbs","title":"Check GarageHQ Backups for k8s DBs","text":"<ul> <li> <p>Login to Rancher</p> </li> <li> <p>Select 'Production Estuary' namespace in top right corner</p> </li> <li> <p>Go 'Workloads' &gt; 'Pods'</p> </li> <li> <p>Open shell in either estuary-api-0 or estuary-api-1 pods:</p> </li> </ul> <p>Enter this command to examine backups:</p> <p><code>envdir \"/run/etc/wal-e.d/env\" wal-g backup-list</code></p>"},{"location":"admin/guides/05-backups-examine/#examine-pgbackrest-backups-for-vm-dbs","title":"Examine pgBackRest Backups for VM DBs","text":""},{"location":"admin/guides/05-backups-examine/#list-pgbackrest-config-stanza-names","title":"List pgbackrest config + stanza names","text":"<p><code>ubuntu@prod-db-backup01:~$ sudo cat /etc/pgbackrest/pgbackrest.conf</code></p>"},{"location":"admin/guides/05-backups-examine/#check-backups-are-functional-for-stanza-fdi_main","title":"Check Backups are functional for stanza fdi_main","text":"<p><code>ubuntu@prod-db-backup01:~$ sudo -u pgbackrest pgbackrest --stanza=fdi_main --log-level-console=info check</code></p>"},{"location":"admin/guides/05-backups-examine/#check-backup-history-for-stanza-fdi_main","title":"Check backup history for stanza fdi_main","text":"<pre><code>ubuntu@prod-db-backup01:~$ sudo -u pgbackrest pgbackrest --stanza=fdi_main --log-level-console=info info\nstanza: fdi_main\n    status: ok\n    cipher: aes-256-cbc\n\n    db (current)\n        wal archive min/max (14): 0000010500000352000000DD/0000010900000362000000A4\n\n        full backup: 20230618-031405F\n            timestamp start/stop: 2023-06-18 03:14:05 / 2023-06-18 03:30:27\n            wal start/stop: 0000010500000352000000DD / 0000010500000352000000DD\n            database size: 211.9GB, database backup size: 211.9GB\n            repo1: backup set size: 61.7GB, backup size: 61.7GB\n\n        full backup: 20230625-031405F\n            timestamp start/stop: 2023-06-25 03:14:05 / 2023-06-25 03:29:58\n            wal start/stop: 000001080000035A000000C4 / 000001080000035A000000C4\n            database size: 213.1GB, database backup size: 213.1GB\n            repo1: backup set size: 62GB, backup size: 62GB\n</code></pre>"},{"location":"admin/guides/06-manual-checks/","title":"FDI Manual Checks","text":"<p>A long list of manual checks that can be done to examine all the important components in FDI.</p>"},{"location":"admin/guides/06-manual-checks/#bastion-servers","title":"Bastion Servers","text":"<p>pcadmin@workstation:~$ for i in 01 02 03; do ssh \"wings@prod-bastion${i}.estuary.tech\" -t \"sudo systemctl status netbird.service &amp;&amp; exit\"; done</p> <p>pcadmin@workstation:~$ for i in 01 02 03; do ssh \"wings@prod-bastion${i}.estuary.tech\" -t \"sudo journalctl -n 25 -u netbird.service &amp;&amp; exit\"; done</p>"},{"location":"admin/guides/06-manual-checks/#ceph","title":"Ceph","text":"<p>https://10.24.0.204:8006/#v1:0:=node%2Faltair:4:38::::::38</p> <p>Check that Ceph is: - OK - Has at least 3 Monitors, Managers and Meta Data Servers - Is not over 80% full - Logs</p>"},{"location":"admin/guides/06-manual-checks/#moosefs","title":"MooseFS","text":"<p>http://mfsmaster.estuary.tech:9425/mfs.cgi</p> <p>Check that the MooseFS:</p> <ul> <li>masters are up!</li> <li>no missing chunks</li> <li>no errors on any Disk</li> <li>chunk servers are OK</li> </ul>"},{"location":"admin/guides/06-manual-checks/#estuary-status-page","title":"Estuary Status Page","text":"<p>https://status.estuary.tech/</p>"},{"location":"admin/guides/06-manual-checks/#checkmk","title":"CheckMK","text":"<p>Examine:</p> <p>https://monitoring.estuary.tech/estuary/check_mk/login.py</p> <p>Green good, red bad! :)</p>"},{"location":"admin/guides/06-manual-checks/#database-clusters","title":"Database Clusters","text":"<p>$ ssh prod-ehi-db01.estuary.tech -t \"sudo patronictl -c /etc/patroni/prod-ehi-db01.estuary.tech.yml list\" + Cluster: prod_ehi_db ------+------------+---------+---------+----+-----------+-----------------+ | Member                     | Host       | Role    | State   | TL | Lag in MB | Pending restart | +----------------------------+------------+---------+---------+----+-----------+-----------------+ | prod-ehi-db01.estuary.tech | 10.24.3.20 | Leader  | running | 76 |           | *               | | prod-ehi-db02.estuary.tech | 10.24.3.21 | Replica | running | 76 |         0 | *               | | prod-ehi-db03.estuary.tech | 10.24.3.22 | Replica | running | 76 |         0 | *               | +----------------------------+------------+---------+---------+----+-----------+-----------------+</p> <p>$ ssh prod-ebi-db01.estuary.tech -t \"sudo patronictl -c /etc/patroni/prod-ebi-db01.estuary.tech.yml list\" + Cluster: prod_ebi_db ------+-----------+---------+---------+----+-----------+ | Member                     | Host      | Role    | State   | TL | Lag in MB | +----------------------------+-----------+---------+---------+----+-----------+ | prod-ebi-db01.estuary.tech | 10.24.3.1 | Leader  | running | 13 |           | | prod-ebi-db02.estuary.tech | 10.24.3.2 | Replica | running |    |        16 | | prod-ebi-db03.estuary.tech | 10.24.3.3 | Replica | running |    |        16 | +----------------------------+-----------+---------+---------+----+-----------+</p>"},{"location":"admin/guides/06-manual-checks/#examine-the-ram-usage","title":"Examine the RAM usage:","text":"<p>$ for i in 01 02 03; do ssh \"prod-ehi-db${i}.estuary.tech\" -t \"free -h\"; done</p> <p>$ for i in 01 02 03; do ssh \"prod-ebi-db${i}.estuary.tech\" -t \"free -h\"; done</p>"},{"location":"admin/guides/06-manual-checks/#examine-the-disk-space-usage","title":"Examine the disk space usage:","text":"<p>$ for i in 01 02 03; do ssh \"prod-ehi-db${i}.estuary.tech\" -t \"df -h\"; done</p> <p>$ for i in 01 02 03; do ssh \"prod-ebi-db${i}.estuary.tech\" -t \"df -h\"; done</p>"},{"location":"admin/guides/06-manual-checks/#haproxy","title":"HAProxy","text":"<p>$ for i in 01 02 03; do ssh \"prod-haproxy${i}.estuary.tech\" -t \"sudo crm_mon -r -1 &amp;&amp; exit\"; done</p> <p>$ for i in 01 02 03; do ssh \"prod-haproxy${i}.estuary.tech\" -t \"sudo pcs status &amp;&amp; exit\"; done</p> <p>$ for i in 01 02 03; do ssh \"prod-haproxy${i}.estuary.tech\" -t \"sudo systemctl status corosync.service &amp;&amp; exit\"; done</p>"},{"location":"admin/guides/06-manual-checks/#delta","title":"Delta","text":"<p>$ for i in 01 02 03 04 05 06 07 08; do ssh \"prod-ehi-delta${i}.estuary.tech\" -t \"sudo systemctl --no-pager status delta.service &amp;&amp; exit\"; done</p>"},{"location":"admin/guides/06-manual-checks/#edge","title":"Edge","text":"<p>$ for i in 01 02 03 04 05 06 07 08; do ssh \"prod-ehi-edge${i}.estuary.tech\" -t \"sudo systemctl --no-pager status edge.service &amp;&amp; exit\"; done</p>"},{"location":"admin/guides/06-manual-checks/#edge-urdi","title":"Edge-urdi","text":"<p>$ for i in 01 02 03 04 05 06 07 08; do ssh \"prod-ehi-edge-urid${i}.estuary.tech\" -t \"sudo systemctl --no-pager status edge.service &amp;&amp; exit\"; done</p>"},{"location":"admin/kubernetes/","title":"Kubernetes and CPI","text":"<p>We have two Kubernetes clusters in FDI. There is the \"parent\" cluster, EBI (Estuary Bootstrap Infrastructure), and the \"child\" cluster, Phosphophyllite (\"phos\").</p>"},{"location":"admin/kubernetes/#logging-into-kubernetes","title":"Logging into Kubernetes","text":"<p>Logging into Kubernetes involves getting an authorisation token and authenticating against the Kubernetes API. The easiest way to do this is using Rancher, which is our Kubernetes management tool of choice.</p> <p>Head to https://rancher.estuary.tech and sign in using the credentials found here.</p> <p>Once you've signed into Rancher, select the cluster you want to administrate by clicking the hamburger menu (\u2630) in the top left, then clicking the name of the cluster (<code>ebi</code> or <code>phosphophyllite</code>). You can now browse and administer the cluster or download KubeConfig files.</p>"},{"location":"admin/kubernetes/#getting-kubectl-access","title":"Getting Kubectl access","text":"<p>Log onto Rancher, select the Kubernetes cluster you want to manage and click \"Download KubeConfig\". </p> <p>Move the file to <code>~/.kube</code> (creating that folder if it doesn't exist), rename it to just <code>config</code>, and make sure it has appropriate permissions - only you should be able to read and write to that folder and the files within it.</p>"},{"location":"admin/kubernetes/#managing-customer-provisioned-infrastructure","title":"Managing Customer Provisioned Infrastructure","text":"<p>CPI is the short name for the overall goal of building, maintaining and managing infrastructure on behalf of FDT's customers.</p> <p>It is powered by:</p> <ul> <li>Helm charts, packaged at https://github.com/application-research/fws-cpi-helm + developed in repos</li> </ul>"},{"location":"admin/kubernetes/#managing-customers","title":"managing customers","text":""},{"location":"admin/kubernetes/#creating-new-customers","title":"creating new customers","text":""},{"location":"admin/kubernetes/#editing-existing-customers","title":"editing existing customers","text":""},{"location":"admin/kubernetes/#chart-development-and-changes","title":"Chart development and changes","text":""},{"location":"admin/kubernetes/#more-details","title":"More details","text":""},{"location":"admin/kubernetes/#rookio-ceph-integration","title":"Rook.io Ceph integration","text":""},{"location":"admin/kubernetes/#argocd-configuration","title":"ArgoCD configuration","text":""},{"location":"admin/kubernetes/#cpi-reporter","title":"CPI Reporter","text":""},{"location":"admin/kubernetes/#the-cpi-automaton","title":"The CPI Automaton","text":""},{"location":"admin/rescue/","title":"Rescue","text":"<p>Rescue covers accessing the infrastructure when All Hope Is Lost, when you need to access things with no other easy way in, or when a physical Groundfloor node is having issues. It covers accessing machines using IPMI (out of band access for when the OS is not responding) and dialing in using Wireguard (for when the Netbird Bastion hosts are unavailable)</p>"},{"location":"admin/services/","title":"Services","text":"<p>Services covers the various services hosted in FDI or that allow one to host things using FDI.</p>"},{"location":"admin/services/#rancher","title":"Rancher","text":""},{"location":"admin/services/#awx","title":"AWX","text":""},{"location":"admin/services/#estuaryv2-edge-delta-edge-urid","title":"EstuaryV2 - Edge, Delta, Edge-URID","text":""},{"location":"admin/services/#carriers","title":"Carriers","text":""},{"location":"admin/services/#shuttle-12","title":"Shuttle 12","text":""},{"location":"admin/services/#api-node","title":"API node","text":""},{"location":"admin/storage/","title":"Storage","text":"<p>Storage covers the four main storage systems in use at FDI, and how to care and feed for them.</p> <p>These include: ZFS (ShuttleRescue), MooseFS, GarageHQ and Ceph.</p>"},{"location":"admin/tls/","title":"TLS","text":"<p>TLS covers all things certificates, and troubleshooting steps for rotating certificates and the like.</p> <ul> <li>Managing TLS - Rotating TLS throughout the infrastructure with Wildcard-TLS-Playbook<ul> <li>Keeping certificates up to date</li> <li>Adding new hosts to distribute TLS certificates to</li> </ul> </li> <li>Managing TLS - Self-signed TLS for PostgreSQL clusters (etcd)</li> </ul>"},{"location":"deployment/running-playbook/","title":"How to run this playbook outside of AWX","text":"<p>This playbook is run with a second private repository and inventory for the private \"group_vars\" files, which are unlocked with various password files. For those of you with enough access, this secondary repository can be viewed at: https://github.com/application-research/estuary-hosted-infrastructure-private</p> <p>The purpose of this seperation is to enhance security and avoid exposing more sensitive and vaulted variables to the wider internet.</p>"},{"location":"deployment/running-playbook/#running-the-playbooks","title":"Running the playbooks","text":"<p>1) Setup aliases. This makes it a lot simpler to run playbooks against the 2 repositories/inventories we use: <pre><code>$ cat ~/.bash_aliases\n# Aliases\nalias ansible-ehi='ansible -u ubuntu -i ~/projects/estuary-hosted-infrastructure/inventories/production-ehi/ -i ~/projects/estuary-hosted-infrastructure-private/inventories/production-ehi/'\nalias ansible-playbook-ehi='ansible-playbook -u ubuntu -i ~/projects/estuary-hosted-infrastructure/inventories/production-ehi/ -i ~/projects/estuary-hosted-infrastructure-private/inventories/production-ehi/'\n</code></pre></p> <p>2) Git clone the two necessary repos with submodules <pre><code>$ cd ~/projects/\n$ git clone git@github.com:application-research/estuary-hosted-infrastructure.git --recurse-submodules\n$ git clone git@github.com:application-research/estuary-hosted-infrastructure-private.git --recurse-submodules\n</code></pre></p> <p>3) Create any necessary Vault secrets locally on your machine (WARNING: we recommend deleting these once done each time, and only doing this on a machine with full disk encryption) <pre><code>$ mkdir -p ~/.ehi-vault\n$ nano ~/.ehi-vault/delta\n$ nano ~/.ehi-vault/loadbalancer\n$ nano ~/.ehi-vault/moosefs\n$ nano ~/.ehi-vault/postgresql\n$ nano ~/.ehi-vault/proxmaas\n$ nano ~/.ehi-vault/tls-materials\n$ nano ~/.ehi-vault/tls-bastion\n</code></pre></p> <p>4) Run the main playbook with every variable file! <pre><code>$ cd ~/projects/estuary-hosted-infrastructure\n$ ansible-playbook-ehi main.yml \\\n--vault-id delta@~/.ehi-vault/delta \\\n--vault-id loadbalancer@~/.ehi-vault/loadbalancer \\\n--vault-id moosefs@~/.ehi-vault/moosefs \\\n--vault-id postgresql@~/.ehi-vault/postgresql \\\n--vault-id proxmaas@~/.ehi-vault/proxmaas \\\n--vault-id tls-bastion@~/.ehi-vault/tls-bastion \\\n--vault-id tls-materials@~/.ehi-vault/tls-materials\n</code></pre></p>"},{"location":"deployment/running-playbook/#how-can-i-only-run-a-specific-sectioninventory","title":"How can I only run a specific section/inventory?","text":"<p>For example, running ehi-proxmaas's \"spawn.yml\" against a specific inventory (dev-haproxy0[1-3]):</p> <pre><code>$ cd ~/projects/estuary-hosted-infrastructure\n$ ansible-playbook-ehi playbooks/ehi-proxmaas/spawn.yml --vault-id proxmaas@~/.ehi-vault/proxmaas  --extra-vars \"machine_details=dev-haproxy\"\n</code></pre> <p>Or run playbooks/logtail.yml against dev-demo[01:03], first make sure you have defined the target inventory in your hosts file: <pre><code>$ cat ~/projects/estuary-hosted-infrastructure/inventories/production-ehi/hosts\n[dev_demo]\ndev-demo[01:03].estuary.tech\n\n[logtail_clients:children]\ndev_demo\n</code></pre> Then run a playbook against it: <pre><code>$ cd ~/projects/estuary-hosted-infrastructure\n$ ansible-playbook-ehi playbooks/logtail-playbook/disconnect.yml\n</code></pre></p>"},{"location":"deployment/running-playbook/#encrypt-a-variable-with-a-vault-password-and-vault-id","title":"Encrypt a variable with a vault password (and vault-id):","text":"<pre><code>$ ansible-vault encrypt_string 'strong-api-token-string' --name 'logtail_api_key' --vault-id proxmaas@prompt\nNew vault password (proxmaas): \nConfirm new vault password (proxmaas): \nEncryption successful\nlogtail_api_key: !vault |\n          $ANSIBLE_VAULT;1.2;AES256;proxmaas\n          11055633918770429142902173396911421216287390132204845285229540464063184442026986\n          66859934371969282116487131834498694708145286012139144133895520666382339655331443\n          57499947727087977380676419368809974229798141346440802729390819747629378816477878\n          52896524828670163974022167202826172166235988258440501501293477735999168369912972\n          53596788072402179773851957224793713770540977348438536151435086987057\n</code></pre>"},{"location":"deployment/s00-01-routerconfigs/","title":"\ud83d\udea7 Stage 00 - Step 1 - MikroTik Initial Router Configuration \ud83d\udea7","text":"<ul> <li>Plug in the MikroTik router.</li> <li>Plug in an out of band serial device to its console port and login.</li> <li>Copy mikrotik-serial.cfg.dist to mikrotik-serial.cfg and fill it out with the settings needed to access the serial console. \ud83d\udea7</li> <li>Copy ip_settings.cfg.dist to ip_settings.cfg and fill it out with the details of your IP address configurations. \ud83d\udea7</li> <li>Run <code>python3 mikrotik.py --initial</code> to apply the initial configuration settings (Might end up as Ansible, probably) \ud83d\udea7</li> </ul> <p>At this point your MikroTik installation is completed, but you will need to set up the core switch + cabling before you can get WAN and LAN working.</p>"},{"location":"deployment/s00-02-switchconfigs/","title":"\ud83d\udea7 Stage 00 - Step 2 - Core switch + BMC switch Configuration \ud83d\udea7","text":"<ul> <li>Plug in the core switch.</li> <li>Plug in an out of band serial device to its console port and login.</li> <li>Copy switch-serial.cfg.dist to switch-serial.cfg and fill it out with the settings needed to access the serial console. \ud83d\udea7</li> <li>Copy switch_settings.cfg.dist to switch_settings.cfg and fill it out with the details of your IP address configurations. \ud83d\udea7</li> <li>Run <code>python3 switches.py --initial</code> to apply the initial configuration settings (Might end up as Ansible, probably) \ud83d\udea7</li> </ul> <p>At this point your switch installation is completed, but you will need to set up the cabling before you can get WAN and LAN working.</p>"},{"location":"deployment/s00-03-physical/","title":"\ud83d\udea7 Under Construction \ud83d\udea7","text":""},{"location":"deployment/s00-04-disable-lacp/","title":"\ud83d\udea7 Under Construction \ud83d\udea7","text":""},{"location":"deployment/s01-01-proxmox/","title":"\ud83d\udea7 Stage 01 - Step 1 - Proxmox installation \ud83d\udea7","text":"<ul> <li>Install the Proxmox nodes. \ud83d\udea7<ul> <li>If you are installing remotely, you can use Wireguard + IPMI to slowly install the first Proxmox node, then setup a HTTP webserver on the node to host a copy of the Proxmox ISO and use that to remotely install the rest of the Proxmox hosts. \ud83d\udea7 TODO: Can probably script that last bit actually.</li> </ul> </li> <li>Cluster the Proxmox nodes \ud83d\udea7 TODO: Script</li> </ul>"},{"location":"deployment/s01-02-ceph/","title":"\ud83d\udea7 Stage 01 - Step 2 - Ceph installation \ud83d\udea7","text":"<ul> <li>This will be an Ansible playbook which uses the Proxmox API to install Ceph on all the nodes and adopt all their disks as OSDs.</li> </ul>"},{"location":"deployment/s02-01-genesis/","title":"\ud83d\udea7 Stage 02 - Step 1 - Genesis installation \ud83d\udea7","text":"<ul> <li>This will be an Ansible playbook that builds an LXC container which will be the Genesis box.</li> <li>It will then kick off a remote job on the Genesis box to git clone this repo and set up everything else.</li> </ul>"},{"location":"deployment/s02-02-dns/","title":"\ud83d\udea7 Stage 02 - Step 2 - DNS installation \ud83d\udea7","text":"<ul> <li>Runs on the Genesis box.</li> <li>Ansible playbook to create the DNS infrastructure and insert/upsert all our DNS records.</li> <li>Python script at the end to update MAAS to use the new DNS servers.</li> </ul>"},{"location":"deployment/s02-03-moosefs/","title":"\ud83d\udea7 Stage 01 - Step 2 - MooseFS installation \ud83d\udea7","text":"<ul> <li>This will be an Ansible playbook which installs the MooseFS infrastructure and adopts all the disks.</li> </ul>"},{"location":"deployment/s02-04-tls-bastion/","title":"\ud83d\udea7 Stage 02 - Step 3 - TLS Bastion \ud83d\udea7","text":"<ul> <li>Runs on the Genesis box.</li> <li>Ansible playbook to create the TLS Bastion host which will set up wildcard TLS certificates and prepare to deploy them.</li> </ul>"},{"location":"deployment/s02-05-haproxy/","title":"\ud83d\udea7 Stage 02 - Step 4 - HAProxy \ud83d\udea7","text":"<ul> <li>Runs on the Genesis box.</li> <li>Ansible playbook to create a highly available HAProxy installation, a prerequisite for many other parts of EBI and EHI.</li> </ul>"},{"location":"deployment/s02-06-ebi-db/","title":"\ud83d\udea7 Stage 02 - Step 6 - EBI PostgreSQL Cluster \ud83d\udea7","text":"<ul> <li>Runs on the Genesis box.</li> <li>Ansible playbook to create a highly available PostgreSQL cluster for EHI to host AWX and Nautobot on.</li> </ul>"},{"location":"deployment/s02-07-ebi-k8s/","title":"\ud83d\udea7 Stage 02 - Step 6 - EBI Kubernetes Cluster \ud83d\udea7","text":"<ul> <li>Runs on the Genesis box.</li> <li>Ansible playbook to create a highly available RKE2 cluster for EBI, then install AWX and Rancher</li> </ul>"},{"location":"deployment/s02-08-bastion/","title":"\ud83d\udea7 Stage 02 - Step 7 - Netbird Bastion Hosts \ud83d\udea7","text":"<ul> <li>Runs on AWX.</li> <li>Ansible playbook to create a highly available set of Bastion hosts for accessing the cluster normally.</li> </ul>"},{"location":"deployment/s02-09-nautobot/","title":"\ud83d\udea7 Stage 02 - Step 9 - Nautobot \ud83d\udea7","text":"<ul> <li>Runs on AWX.</li> <li>Ansible playbook to create a highly available set of Nautobot hosts and setup Nautobot.</li> </ul>"},{"location":"deployment/s03-01-maas/","title":"\ud83d\udea7 Stage 03 - Step 01 - Metal as a Service \ud83d\udea7","text":"<ul> <li>Runs on AWX</li> <li>Creates the MAAS infrastructure, migrates a copy of the MAAS databases to itself, creates the second and third MAAS nodes using that new database, migrates Genesis to 10.24.137.137, then removes MAAS from it and creates the proper first MAAS node at its correct IP and sets up the final MAAS nodes.</li> </ul>"},{"location":"deployment/s04-01-ehi-db/","title":"\ud83d\udea7 Stage 04 - Step 1 - EHI PostgreSQL Cluster \ud83d\udea7","text":"<ul> <li>Runs on AWX.</li> <li>Ansible playbook to create a highly available PostgreSQL cluster for EHI to host main services on (besides API).</li> </ul>"},{"location":"deployment/s04-02-ehi-api-db/","title":"\ud83d\udea7 Stage 04 - Step 2 - EHI API PostgreSQL Cluster \ud83d\udea7","text":"<ul> <li>Runs on AWX.</li> <li>Ansible playbook to create a highly available PostgreSQL cluster for EHI to host API services on.</li> </ul>"},{"location":"deployment/s04-03-ehi-api-nsq/","title":"\ud83d\udea7 Stage 04 - Step 3 - EHI NSQ Cluster \ud83d\udea7","text":"<ul> <li>Runs on AWX.</li> <li>Ansible playbook to create a highly available nsq cluster for EHI to host API services on.</li> </ul>"},{"location":"deployment/s04-04-ehi-api/","title":"\ud83d\udea7 Stage 04 - Step 4 - EHI Highly Available API (HAAPI) Cluster \ud83d\udea7","text":"<ul> <li>Runs on AWX.</li> <li>Ansible playbook to create a highly available nsq cluster for EHI to host API services on.</li> </ul>"},{"location":"deployment/s04-05-ehi-edge/","title":"\ud83d\udea7 Stage 04 - Step 5 - Edge Cluster \ud83d\udea7","text":"<ul> <li>Runs on AWX.</li> <li>Ansible playbook to create a highly available Edge-UR cluster.</li> </ul>"},{"location":"deployment/s04-06-phos-k8s/","title":"\ud83d\udea7 Stage 04 - Step 6 - Phosphophyllite Kubernetes \ud83d\udea7","text":"<ul> <li>Runs on AWX.</li> <li>Ansible playbook to create Phosphopyllite, a production EHI RKE2 Kubernetes cluster.</li> </ul>"}]}